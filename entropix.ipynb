{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xjdr-alt/entropix/blob/main/entropix.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "xZd9EBLmgKU6"
      },
      "outputs": [],
      "source": [
        "from typing import Dict, List, NamedTuple, Optional, Tuple\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-v7mNYwHnPJz"
      },
      "source": [
        "# Set Model ID and Token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "oHaZYRqAnRPS"
      },
      "outputs": [],
      "source": [
        "MODEL_ID = 'meta-llama/Llama-3.2-1B-Instruct'\n",
        "TOKEN = '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'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2IdvdlSltd9"
      },
      "source": [
        "# Config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "RHxP8Bd1lvo8"
      },
      "outputs": [],
      "source": [
        "params = {\n",
        "  \"dim\": 2048,\n",
        "  \"n_layers\": 16,\n",
        "  \"n_heads\": 32,\n",
        "  \"n_kv_heads\": 8,\n",
        "  \"vocab_size\": 128256,\n",
        "  \"ffn_dim_multiplier\": 1.5,\n",
        "  \"multiple_of\": 256,\n",
        "  \"norm_eps\": 1e-05,\n",
        "  \"rope_theta\": 500000.0,\n",
        "  \"use_scaled_rope\": True,\n",
        "  \"max_seq_len\": 4096\n",
        "}\n",
        "\n",
        "\n",
        "class ModelParams(NamedTuple):\n",
        "  n_layers: int\n",
        "  n_local_heads: int\n",
        "  n_local_kv_heads: int\n",
        "  head_dim: int\n",
        "  max_seq_len: int\n",
        "  rope_theta: float\n",
        "  use_scaled_rope: bool\n",
        "\n",
        "\n",
        "LLAMA_1B_PARAMS = ModelParams(\n",
        "  n_layers=params[\"n_layers\"],\n",
        "  n_local_heads=params[\"n_heads\"],\n",
        "  n_local_kv_heads=params[\"n_kv_heads\"],\n",
        "  head_dim=params[\"dim\"] // params[\"n_heads\"],\n",
        "  max_seq_len=params[\"max_seq_len\"],\n",
        "  rope_theta=params[\"rope_theta\"],\n",
        "  use_scaled_rope=params[\"use_scaled_rope\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F0fWAb83ghC1"
      },
      "source": [
        "# Download Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "flDeKnQlhS1J"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import ml_dtypes\n",
        "from pathlib import Path\n",
        "\n",
        "from transformers import AutoModelForCausalLM\n",
        "from unittest.mock import patch\n",
        "from transformers.dynamic_module_utils import get_imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "u3lTK6HWhbFV"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " model.embed_tokens.weight: param.shape=torch.Size([128256, 2048])\n",
            "Writing model.embed_tokens.weight as tok_embeddings.weight to weights/1B-Instruct/tok_embeddings.weight.npy\n",
            " model.layers.0.self_attn.q_proj.weight: param.shape=torch.Size([2048, 2048])\n",
            "Writing model.layers.0.self_attn.q_proj.weight as layers.0.attention.wq.weight to weights/1B-Instruct/layers.0.attention.wq.weight.npy\n",
            " model.layers.0.self_attn.k_proj.weight: param.shape=torch.Size([512, 2048])\n",
            "Writing model.layers.0.self_attn.k_proj.weight as layers.0.attention.wk.weight to weights/1B-Instruct/layers.0.attention.wk.weight.npy\n",
            " model.layers.0.self_attn.v_proj.weight: param.shape=torch.Size([512, 2048])\n",
            "Writing model.layers.0.self_attn.v_proj.weight as layers.0.attention.wv.weight to weights/1B-Instruct/layers.0.attention.wv.weight.npy\n",
            " model.layers.0.self_attn.o_proj.weight: param.shape=torch.Size([2048, 2048])\n",
            "Writing model.layers.0.self_attn.o_proj.weight as layers.0.attention.wo.weight to weights/1B-Instruct/layers.0.attention.wo.weight.npy\n",
            " model.layers.0.mlp.gate_proj.weight: param.shape=torch.Size([8192, 2048])\n",
            "Writing model.layers.0.mlp.gate_proj.weight as layers.0.feed_forward.w1.weight to weights/1B-Instruct/layers.0.feed_forward.w1.weight.npy\n",
            " model.layers.0.mlp.up_proj.weight: param.shape=torch.Size([8192, 2048])\n",
            "Writing model.layers.0.mlp.up_proj.weight as layers.0.feed_forward.w3.weight to weights/1B-Instruct/layers.0.feed_forward.w3.weight.npy\n",
            " model.layers.0.mlp.down_proj.weight: param.shape=torch.Size([2048, 8192])\n",
            "Writing model.layers.0.mlp.down_proj.weight as layers.0.feed_forward.w2.weight to weights/1B-Instruct/layers.0.feed_forward.w2.weight.npy\n",
            " model.layers.0.input_layernorm.weight: param.shape=torch.Size([2048])\n",
            "Writing model.layers.0.input_layernorm.weight as layers.0.attention_norm.weight to weights/1B-Instruct/layers.0.attention_norm.weight.npy\n",
            " model.layers.0.post_attention_layernorm.weight: param.shape=torch.Size([2048])\n",
            "Writing model.layers.0.post_attention_layernorm.weight as layers.0.ffn_norm.weight to weights/1B-Instruct/layers.0.ffn_norm.weight.npy\n",
            " model.layers.1.self_attn.q_proj.weight: param.shape=torch.Size([2048, 2048])\n",
            "Writing model.layers.1.self_attn.q_proj.weight as layers.1.attention.wq.weight to weights/1B-Instruct/layers.1.attention.wq.weight.npy\n",
            " model.layers.1.self_attn.k_proj.weight: param.shape=torch.Size([512, 2048])\n",
            "Writing model.layers.1.self_attn.k_proj.weight as layers.1.attention.wk.weight to weights/1B-Instruct/layers.1.attention.wk.weight.npy\n",
            " model.layers.1.self_attn.v_proj.weight: param.shape=torch.Size([512, 2048])\n",
            "Writing model.layers.1.self_attn.v_proj.weight as layers.1.attention.wv.weight to weights/1B-Instruct/layers.1.attention.wv.weight.npy\n",
            " model.layers.1.self_attn.o_proj.weight: param.shape=torch.Size([2048, 2048])\n",
            "Writing model.layers.1.self_attn.o_proj.weight as layers.1.attention.wo.weight to weights/1B-Instruct/layers.1.attention.wo.weight.npy\n",
            " model.layers.1.mlp.gate_proj.weight: param.shape=torch.Size([8192, 2048])\n",
            "Writing model.layers.1.mlp.gate_proj.weight as layers.1.feed_forward.w1.weight to weights/1B-Instruct/layers.1.feed_forward.w1.weight.npy\n",
            " model.layers.1.mlp.up_proj.weight: param.shape=torch.Size([8192, 2048])\n",
            "Writing model.layers.1.mlp.up_proj.weight as layers.1.feed_forward.w3.weight to weights/1B-Instruct/layers.1.feed_forward.w3.weight.npy\n",
            " model.layers.1.mlp.down_proj.weight: param.shape=torch.Size([2048, 8192])\n",
            "Writing model.layers.1.mlp.down_proj.weight as layers.1.feed_forward.w2.weight to weights/1B-Instruct/layers.1.feed_forward.w2.weight.npy\n",
            " model.layers.1.input_layernorm.weight: param.shape=torch.Size([2048])\n",
            "Writing model.layers.1.input_layernorm.weight as layers.1.attention_norm.weight to weights/1B-Instruct/layers.1.attention_norm.weight.npy\n",
            " model.layers.1.post_attention_layernorm.weight: param.shape=torch.Size([2048])\n",
            "Writing model.layers.1.post_attention_layernorm.weight as layers.1.ffn_norm.weight to weights/1B-Instruct/layers.1.ffn_norm.weight.npy\n",
            " model.layers.2.self_attn.q_proj.weight: param.shape=torch.Size([2048, 2048])\n",
            "Writing model.layers.2.self_attn.q_proj.weight as layers.2.attention.wq.weight to weights/1B-Instruct/layers.2.attention.wq.weight.npy\n",
            " model.layers.2.self_attn.k_proj.weight: param.shape=torch.Size([512, 2048])\n",
            "Writing model.layers.2.self_attn.k_proj.weight as layers.2.attention.wk.weight to weights/1B-Instruct/layers.2.attention.wk.weight.npy\n",
            " model.layers.2.self_attn.v_proj.weight: param.shape=torch.Size([512, 2048])\n",
            "Writing model.layers.2.self_attn.v_proj.weight as layers.2.attention.wv.weight to weights/1B-Instruct/layers.2.attention.wv.weight.npy\n",
            " model.layers.2.self_attn.o_proj.weight: param.shape=torch.Size([2048, 2048])\n",
            "Writing model.layers.2.self_attn.o_proj.weight as layers.2.attention.wo.weight to weights/1B-Instruct/layers.2.attention.wo.weight.npy\n",
            " model.layers.2.mlp.gate_proj.weight: param.shape=torch.Size([8192, 2048])\n",
            "Writing model.layers.2.mlp.gate_proj.weight as layers.2.feed_forward.w1.weight to weights/1B-Instruct/layers.2.feed_forward.w1.weight.npy\n",
            " model.layers.2.mlp.up_proj.weight: param.shape=torch.Size([8192, 2048])\n",
            "Writing model.layers.2.mlp.up_proj.weight as layers.2.feed_forward.w3.weight to weights/1B-Instruct/layers.2.feed_forward.w3.weight.npy\n",
            " model.layers.2.mlp.down_proj.weight: param.shape=torch.Size([2048, 8192])\n",
            "Writing model.layers.2.mlp.down_proj.weight as layers.2.feed_forward.w2.weight to weights/1B-Instruct/layers.2.feed_forward.w2.weight.npy\n",
            " model.layers.2.input_layernorm.weight: param.shape=torch.Size([2048])\n",
            "Writing model.layers.2.input_layernorm.weight as layers.2.attention_norm.weight to weights/1B-Instruct/layers.2.attention_norm.weight.npy\n",
            " model.layers.2.post_attention_layernorm.weight: param.shape=torch.Size([2048])\n",
            "Writing model.layers.2.post_attention_layernorm.weight as layers.2.ffn_norm.weight to weights/1B-Instruct/layers.2.ffn_norm.weight.npy\n",
            " model.layers.3.self_attn.q_proj.weight: param.shape=torch.Size([2048, 2048])\n",
            "Writing model.layers.3.self_attn.q_proj.weight as layers.3.attention.wq.weight to weights/1B-Instruct/layers.3.attention.wq.weight.npy\n",
            " model.layers.3.self_attn.k_proj.weight: param.shape=torch.Size([512, 2048])\n",
            "Writing model.layers.3.self_attn.k_proj.weight as layers.3.attention.wk.weight to weights/1B-Instruct/layers.3.attention.wk.weight.npy\n",
            " model.layers.3.self_attn.v_proj.weight: param.shape=torch.Size([512, 2048])\n",
            "Writing model.layers.3.self_attn.v_proj.weight as layers.3.attention.wv.weight to weights/1B-Instruct/layers.3.attention.wv.weight.npy\n",
            " model.layers.3.self_attn.o_proj.weight: param.shape=torch.Size([2048, 2048])\n",
            "Writing model.layers.3.self_attn.o_proj.weight as layers.3.attention.wo.weight to weights/1B-Instruct/layers.3.attention.wo.weight.npy\n",
            " model.layers.3.mlp.gate_proj.weight: param.shape=torch.Size([8192, 2048])\n",
            "Writing model.layers.3.mlp.gate_proj.weight as layers.3.feed_forward.w1.weight to weights/1B-Instruct/layers.3.feed_forward.w1.weight.npy\n",
            " model.layers.3.mlp.up_proj.weight: param.shape=torch.Size([8192, 2048])\n",
            "Writing model.layers.3.mlp.up_proj.weight as layers.3.feed_forward.w3.weight to weights/1B-Instruct/layers.3.feed_forward.w3.weight.npy\n",
            " model.layers.3.mlp.down_proj.weight: param.shape=torch.Size([2048, 8192])\n",
            "Writing model.layers.3.mlp.down_proj.weight as layers.3.feed_forward.w2.weight to weights/1B-Instruct/layers.3.feed_forward.w2.weight.npy\n",
            " model.layers.3.input_layernorm.weight: param.shape=torch.Size([2048])\n",
            "Writing model.layers.3.input_layernorm.weight as layers.3.attention_norm.weight to weights/1B-Instruct/layers.3.attention_norm.weight.npy\n",
            " model.layers.3.post_attention_layernorm.weight: param.shape=torch.Size([2048])\n",
            "Writing model.layers.3.post_attention_layernorm.weight as layers.3.ffn_norm.weight to weights/1B-Instruct/layers.3.ffn_norm.weight.npy\n",
            " model.layers.4.self_attn.q_proj.weight: param.shape=torch.Size([2048, 2048])\n",
            "Writing model.layers.4.self_attn.q_proj.weight as layers.4.attention.wq.weight to weights/1B-Instruct/layers.4.attention.wq.weight.npy\n",
            " model.layers.4.self_attn.k_proj.weight: param.shape=torch.Size([512, 2048])\n",
            "Writing model.layers.4.self_attn.k_proj.weight as layers.4.attention.wk.weight to weights/1B-Instruct/layers.4.attention.wk.weight.npy\n",
            " model.layers.4.self_attn.v_proj.weight: param.shape=torch.Size([512, 2048])\n",
            "Writing model.layers.4.self_attn.v_proj.weight as layers.4.attention.wv.weight to weights/1B-Instruct/layers.4.attention.wv.weight.npy\n",
            " model.layers.4.self_attn.o_proj.weight: param.shape=torch.Size([2048, 2048])\n",
            "Writing model.layers.4.self_attn.o_proj.weight as layers.4.attention.wo.weight to weights/1B-Instruct/layers.4.attention.wo.weight.npy\n",
            " model.layers.4.mlp.gate_proj.weight: param.shape=torch.Size([8192, 2048])\n",
            "Writing model.layers.4.mlp.gate_proj.weight as layers.4.feed_forward.w1.weight to weights/1B-Instruct/layers.4.feed_forward.w1.weight.npy\n",
            " model.layers.4.mlp.up_proj.weight: param.shape=torch.Size([8192, 2048])\n",
            "Writing model.layers.4.mlp.up_proj.weight as layers.4.feed_forward.w3.weight to weights/1B-Instruct/layers.4.feed_forward.w3.weight.npy\n",
            " model.layers.4.mlp.down_proj.weight: param.shape=torch.Size([2048, 8192])\n",
            "Writing model.layers.4.mlp.down_proj.weight as layers.4.feed_forward.w2.weight to weights/1B-Instruct/layers.4.feed_forward.w2.weight.npy\n",
            " model.layers.4.input_layernorm.weight: param.shape=torch.Size([2048])\n",
            "Writing model.layers.4.input_layernorm.weight as layers.4.attention_norm.weight to weights/1B-Instruct/layers.4.attention_norm.weight.npy\n",
            " model.layers.4.post_attention_layernorm.weight: param.shape=torch.Size([2048])\n",
            "Writing model.layers.4.post_attention_layernorm.weight as layers.4.ffn_norm.weight to weights/1B-Instruct/layers.4.ffn_norm.weight.npy\n",
            " model.layers.5.self_attn.q_proj.weight: param.shape=torch.Size([2048, 2048])\n",
            "Writing model.layers.5.self_attn.q_proj.weight as layers.5.attention.wq.weight to weights/1B-Instruct/layers.5.attention.wq.weight.npy\n",
            " model.layers.5.self_attn.k_proj.weight: param.shape=torch.Size([512, 2048])\n",
            "Writing model.layers.5.self_attn.k_proj.weight as layers.5.attention.wk.weight to weights/1B-Instruct/layers.5.attention.wk.weight.npy\n",
            " model.layers.5.self_attn.v_proj.weight: param.shape=torch.Size([512, 2048])\n",
            "Writing model.layers.5.self_attn.v_proj.weight as layers.5.attention.wv.weight to weights/1B-Instruct/layers.5.attention.wv.weight.npy\n",
            " model.layers.5.self_attn.o_proj.weight: param.shape=torch.Size([2048, 2048])\n",
            "Writing model.layers.5.self_attn.o_proj.weight as layers.5.attention.wo.weight to weights/1B-Instruct/layers.5.attention.wo.weight.npy\n",
            " model.layers.5.mlp.gate_proj.weight: param.shape=torch.Size([8192, 2048])\n",
            "Writing model.layers.5.mlp.gate_proj.weight as layers.5.feed_forward.w1.weight to weights/1B-Instruct/layers.5.feed_forward.w1.weight.npy\n",
            " model.layers.5.mlp.up_proj.weight: param.shape=torch.Size([8192, 2048])\n",
            "Writing model.layers.5.mlp.up_proj.weight as layers.5.feed_forward.w3.weight to weights/1B-Instruct/layers.5.feed_forward.w3.weight.npy\n",
            " model.layers.5.mlp.down_proj.weight: param.shape=torch.Size([2048, 8192])\n",
            "Writing model.layers.5.mlp.down_proj.weight as layers.5.feed_forward.w2.weight to weights/1B-Instruct/layers.5.feed_forward.w2.weight.npy\n",
            " model.layers.5.input_layernorm.weight: param.shape=torch.Size([2048])\n",
            "Writing model.layers.5.input_layernorm.weight as layers.5.attention_norm.weight to weights/1B-Instruct/layers.5.attention_norm.weight.npy\n",
            " model.layers.5.post_attention_layernorm.weight: param.shape=torch.Size([2048])\n",
            "Writing model.layers.5.post_attention_layernorm.weight as layers.5.ffn_norm.weight to weights/1B-Instruct/layers.5.ffn_norm.weight.npy\n",
            " model.layers.6.self_attn.q_proj.weight: param.shape=torch.Size([2048, 2048])\n",
            "Writing model.layers.6.self_attn.q_proj.weight as layers.6.attention.wq.weight to weights/1B-Instruct/layers.6.attention.wq.weight.npy\n",
            " model.layers.6.self_attn.k_proj.weight: param.shape=torch.Size([512, 2048])\n",
            "Writing model.layers.6.self_attn.k_proj.weight as layers.6.attention.wk.weight to weights/1B-Instruct/layers.6.attention.wk.weight.npy\n",
            " model.layers.6.self_attn.v_proj.weight: param.shape=torch.Size([512, 2048])\n",
            "Writing model.layers.6.self_attn.v_proj.weight as layers.6.attention.wv.weight to weights/1B-Instruct/layers.6.attention.wv.weight.npy\n",
            " model.layers.6.self_attn.o_proj.weight: param.shape=torch.Size([2048, 2048])\n",
            "Writing model.layers.6.self_attn.o_proj.weight as layers.6.attention.wo.weight to weights/1B-Instruct/layers.6.attention.wo.weight.npy\n",
            " model.layers.6.mlp.gate_proj.weight: param.shape=torch.Size([8192, 2048])\n",
            "Writing model.layers.6.mlp.gate_proj.weight as layers.6.feed_forward.w1.weight to weights/1B-Instruct/layers.6.feed_forward.w1.weight.npy\n",
            " model.layers.6.mlp.up_proj.weight: param.shape=torch.Size([8192, 2048])\n",
            "Writing model.layers.6.mlp.up_proj.weight as layers.6.feed_forward.w3.weight to weights/1B-Instruct/layers.6.feed_forward.w3.weight.npy\n",
            " model.layers.6.mlp.down_proj.weight: param.shape=torch.Size([2048, 8192])\n",
            "Writing model.layers.6.mlp.down_proj.weight as layers.6.feed_forward.w2.weight to weights/1B-Instruct/layers.6.feed_forward.w2.weight.npy\n",
            " model.layers.6.input_layernorm.weight: param.shape=torch.Size([2048])\n",
            "Writing model.layers.6.input_layernorm.weight as layers.6.attention_norm.weight to weights/1B-Instruct/layers.6.attention_norm.weight.npy\n",
            " model.layers.6.post_attention_layernorm.weight: param.shape=torch.Size([2048])\n",
            "Writing model.layers.6.post_attention_layernorm.weight as layers.6.ffn_norm.weight to weights/1B-Instruct/layers.6.ffn_norm.weight.npy\n",
            " model.layers.7.self_attn.q_proj.weight: param.shape=torch.Size([2048, 2048])\n",
            "Writing model.layers.7.self_attn.q_proj.weight as layers.7.attention.wq.weight to weights/1B-Instruct/layers.7.attention.wq.weight.npy\n",
            " model.layers.7.self_attn.k_proj.weight: param.shape=torch.Size([512, 2048])\n",
            "Writing model.layers.7.self_attn.k_proj.weight as layers.7.attention.wk.weight to weights/1B-Instruct/layers.7.attention.wk.weight.npy\n",
            " model.layers.7.self_attn.v_proj.weight: param.shape=torch.Size([512, 2048])\n",
            "Writing model.layers.7.self_attn.v_proj.weight as layers.7.attention.wv.weight to weights/1B-Instruct/layers.7.attention.wv.weight.npy\n",
            " model.layers.7.self_attn.o_proj.weight: param.shape=torch.Size([2048, 2048])\n",
            "Writing model.layers.7.self_attn.o_proj.weight as layers.7.attention.wo.weight to weights/1B-Instruct/layers.7.attention.wo.weight.npy\n",
            " model.layers.7.mlp.gate_proj.weight: param.shape=torch.Size([8192, 2048])\n",
            "Writing model.layers.7.mlp.gate_proj.weight as layers.7.feed_forward.w1.weight to weights/1B-Instruct/layers.7.feed_forward.w1.weight.npy\n",
            " model.layers.7.mlp.up_proj.weight: param.shape=torch.Size([8192, 2048])\n",
            "Writing model.layers.7.mlp.up_proj.weight as layers.7.feed_forward.w3.weight to weights/1B-Instruct/layers.7.feed_forward.w3.weight.npy\n",
            " model.layers.7.mlp.down_proj.weight: param.shape=torch.Size([2048, 8192])\n",
            "Writing model.layers.7.mlp.down_proj.weight as layers.7.feed_forward.w2.weight to weights/1B-Instruct/layers.7.feed_forward.w2.weight.npy\n",
            " model.layers.7.input_layernorm.weight: param.shape=torch.Size([2048])\n",
            "Writing model.layers.7.input_layernorm.weight as layers.7.attention_norm.weight to weights/1B-Instruct/layers.7.attention_norm.weight.npy\n",
            " model.layers.7.post_attention_layernorm.weight: param.shape=torch.Size([2048])\n",
            "Writing model.layers.7.post_attention_layernorm.weight as layers.7.ffn_norm.weight to weights/1B-Instruct/layers.7.ffn_norm.weight.npy\n",
            " model.layers.8.self_attn.q_proj.weight: param.shape=torch.Size([2048, 2048])\n",
            "Writing model.layers.8.self_attn.q_proj.weight as layers.8.attention.wq.weight to weights/1B-Instruct/layers.8.attention.wq.weight.npy\n",
            " model.layers.8.self_attn.k_proj.weight: param.shape=torch.Size([512, 2048])\n",
            "Writing model.layers.8.self_attn.k_proj.weight as layers.8.attention.wk.weight to weights/1B-Instruct/layers.8.attention.wk.weight.npy\n",
            " model.layers.8.self_attn.v_proj.weight: param.shape=torch.Size([512, 2048])\n",
            "Writing model.layers.8.self_attn.v_proj.weight as layers.8.attention.wv.weight to weights/1B-Instruct/layers.8.attention.wv.weight.npy\n",
            " model.layers.8.self_attn.o_proj.weight: param.shape=torch.Size([2048, 2048])\n",
            "Writing model.layers.8.self_attn.o_proj.weight as layers.8.attention.wo.weight to weights/1B-Instruct/layers.8.attention.wo.weight.npy\n",
            " model.layers.8.mlp.gate_proj.weight: param.shape=torch.Size([8192, 2048])\n",
            "Writing model.layers.8.mlp.gate_proj.weight as layers.8.feed_forward.w1.weight to weights/1B-Instruct/layers.8.feed_forward.w1.weight.npy\n",
            " model.layers.8.mlp.up_proj.weight: param.shape=torch.Size([8192, 2048])\n",
            "Writing model.layers.8.mlp.up_proj.weight as layers.8.feed_forward.w3.weight to weights/1B-Instruct/layers.8.feed_forward.w3.weight.npy\n",
            " model.layers.8.mlp.down_proj.weight: param.shape=torch.Size([2048, 8192])\n",
            "Writing model.layers.8.mlp.down_proj.weight as layers.8.feed_forward.w2.weight to weights/1B-Instruct/layers.8.feed_forward.w2.weight.npy\n",
            " model.layers.8.input_layernorm.weight: param.shape=torch.Size([2048])\n",
            "Writing model.layers.8.input_layernorm.weight as layers.8.attention_norm.weight to weights/1B-Instruct/layers.8.attention_norm.weight.npy\n",
            " model.layers.8.post_attention_layernorm.weight: param.shape=torch.Size([2048])\n",
            "Writing model.layers.8.post_attention_layernorm.weight as layers.8.ffn_norm.weight to weights/1B-Instruct/layers.8.ffn_norm.weight.npy\n",
            " model.layers.9.self_attn.q_proj.weight: param.shape=torch.Size([2048, 2048])\n",
            "Writing model.layers.9.self_attn.q_proj.weight as layers.9.attention.wq.weight to weights/1B-Instruct/layers.9.attention.wq.weight.npy\n",
            " model.layers.9.self_attn.k_proj.weight: param.shape=torch.Size([512, 2048])\n",
            "Writing model.layers.9.self_attn.k_proj.weight as layers.9.attention.wk.weight to weights/1B-Instruct/layers.9.attention.wk.weight.npy\n",
            " model.layers.9.self_attn.v_proj.weight: param.shape=torch.Size([512, 2048])\n",
            "Writing model.layers.9.self_attn.v_proj.weight as layers.9.attention.wv.weight to weights/1B-Instruct/layers.9.attention.wv.weight.npy\n",
            " model.layers.9.self_attn.o_proj.weight: param.shape=torch.Size([2048, 2048])\n",
            "Writing model.layers.9.self_attn.o_proj.weight as layers.9.attention.wo.weight to weights/1B-Instruct/layers.9.attention.wo.weight.npy\n",
            " model.layers.9.mlp.gate_proj.weight: param.shape=torch.Size([8192, 2048])\n",
            "Writing model.layers.9.mlp.gate_proj.weight as layers.9.feed_forward.w1.weight to weights/1B-Instruct/layers.9.feed_forward.w1.weight.npy\n",
            " model.layers.9.mlp.up_proj.weight: param.shape=torch.Size([8192, 2048])\n",
            "Writing model.layers.9.mlp.up_proj.weight as layers.9.feed_forward.w3.weight to weights/1B-Instruct/layers.9.feed_forward.w3.weight.npy\n",
            " model.layers.9.mlp.down_proj.weight: param.shape=torch.Size([2048, 8192])\n",
            "Writing model.layers.9.mlp.down_proj.weight as layers.9.feed_forward.w2.weight to weights/1B-Instruct/layers.9.feed_forward.w2.weight.npy\n",
            " model.layers.9.input_layernorm.weight: param.shape=torch.Size([2048])\n",
            "Writing model.layers.9.input_layernorm.weight as layers.9.attention_norm.weight to weights/1B-Instruct/layers.9.attention_norm.weight.npy\n",
            " model.layers.9.post_attention_layernorm.weight: param.shape=torch.Size([2048])\n",
            "Writing model.layers.9.post_attention_layernorm.weight as layers.9.ffn_norm.weight to weights/1B-Instruct/layers.9.ffn_norm.weight.npy\n",
            " model.layers.10.self_attn.q_proj.weight: param.shape=torch.Size([2048, 2048])\n",
            "Writing model.layers.10.self_attn.q_proj.weight as layers.10.attention.wq.weight to weights/1B-Instruct/layers.10.attention.wq.weight.npy\n",
            " model.layers.10.self_attn.k_proj.weight: param.shape=torch.Size([512, 2048])\n",
            "Writing model.layers.10.self_attn.k_proj.weight as layers.10.attention.wk.weight to weights/1B-Instruct/layers.10.attention.wk.weight.npy\n",
            " model.layers.10.self_attn.v_proj.weight: param.shape=torch.Size([512, 2048])\n",
            "Writing model.layers.10.self_attn.v_proj.weight as layers.10.attention.wv.weight to weights/1B-Instruct/layers.10.attention.wv.weight.npy\n",
            " model.layers.10.self_attn.o_proj.weight: param.shape=torch.Size([2048, 2048])\n",
            "Writing model.layers.10.self_attn.o_proj.weight as layers.10.attention.wo.weight to weights/1B-Instruct/layers.10.attention.wo.weight.npy\n",
            " model.layers.10.mlp.gate_proj.weight: param.shape=torch.Size([8192, 2048])\n",
            "Writing model.layers.10.mlp.gate_proj.weight as layers.10.feed_forward.w1.weight to weights/1B-Instruct/layers.10.feed_forward.w1.weight.npy\n",
            " model.layers.10.mlp.up_proj.weight: param.shape=torch.Size([8192, 2048])\n",
            "Writing model.layers.10.mlp.up_proj.weight as layers.10.feed_forward.w3.weight to weights/1B-Instruct/layers.10.feed_forward.w3.weight.npy\n",
            " model.layers.10.mlp.down_proj.weight: param.shape=torch.Size([2048, 8192])\n",
            "Writing model.layers.10.mlp.down_proj.weight as layers.10.feed_forward.w2.weight to weights/1B-Instruct/layers.10.feed_forward.w2.weight.npy\n",
            " model.layers.10.input_layernorm.weight: param.shape=torch.Size([2048])\n",
            "Writing model.layers.10.input_layernorm.weight as layers.10.attention_norm.weight to weights/1B-Instruct/layers.10.attention_norm.weight.npy\n",
            " model.layers.10.post_attention_layernorm.weight: param.shape=torch.Size([2048])\n",
            "Writing model.layers.10.post_attention_layernorm.weight as layers.10.ffn_norm.weight to weights/1B-Instruct/layers.10.ffn_norm.weight.npy\n",
            " model.layers.11.self_attn.q_proj.weight: param.shape=torch.Size([2048, 2048])\n",
            "Writing model.layers.11.self_attn.q_proj.weight as layers.11.attention.wq.weight to weights/1B-Instruct/layers.11.attention.wq.weight.npy\n",
            " model.layers.11.self_attn.k_proj.weight: param.shape=torch.Size([512, 2048])\n",
            "Writing model.layers.11.self_attn.k_proj.weight as layers.11.attention.wk.weight to weights/1B-Instruct/layers.11.attention.wk.weight.npy\n",
            " model.layers.11.self_attn.v_proj.weight: param.shape=torch.Size([512, 2048])\n",
            "Writing model.layers.11.self_attn.v_proj.weight as layers.11.attention.wv.weight to weights/1B-Instruct/layers.11.attention.wv.weight.npy\n",
            " model.layers.11.self_attn.o_proj.weight: param.shape=torch.Size([2048, 2048])\n",
            "Writing model.layers.11.self_attn.o_proj.weight as layers.11.attention.wo.weight to weights/1B-Instruct/layers.11.attention.wo.weight.npy\n",
            " model.layers.11.mlp.gate_proj.weight: param.shape=torch.Size([8192, 2048])\n",
            "Writing model.layers.11.mlp.gate_proj.weight as layers.11.feed_forward.w1.weight to weights/1B-Instruct/layers.11.feed_forward.w1.weight.npy\n",
            " model.layers.11.mlp.up_proj.weight: param.shape=torch.Size([8192, 2048])\n",
            "Writing model.layers.11.mlp.up_proj.weight as layers.11.feed_forward.w3.weight to weights/1B-Instruct/layers.11.feed_forward.w3.weight.npy\n",
            " model.layers.11.mlp.down_proj.weight: param.shape=torch.Size([2048, 8192])\n",
            "Writing model.layers.11.mlp.down_proj.weight as layers.11.feed_forward.w2.weight to weights/1B-Instruct/layers.11.feed_forward.w2.weight.npy\n",
            " model.layers.11.input_layernorm.weight: param.shape=torch.Size([2048])\n",
            "Writing model.layers.11.input_layernorm.weight as layers.11.attention_norm.weight to weights/1B-Instruct/layers.11.attention_norm.weight.npy\n",
            " model.layers.11.post_attention_layernorm.weight: param.shape=torch.Size([2048])\n",
            "Writing model.layers.11.post_attention_layernorm.weight as layers.11.ffn_norm.weight to weights/1B-Instruct/layers.11.ffn_norm.weight.npy\n",
            " model.layers.12.self_attn.q_proj.weight: param.shape=torch.Size([2048, 2048])\n",
            "Writing model.layers.12.self_attn.q_proj.weight as layers.12.attention.wq.weight to weights/1B-Instruct/layers.12.attention.wq.weight.npy\n",
            " model.layers.12.self_attn.k_proj.weight: param.shape=torch.Size([512, 2048])\n",
            "Writing model.layers.12.self_attn.k_proj.weight as layers.12.attention.wk.weight to weights/1B-Instruct/layers.12.attention.wk.weight.npy\n",
            " model.layers.12.self_attn.v_proj.weight: param.shape=torch.Size([512, 2048])\n",
            "Writing model.layers.12.self_attn.v_proj.weight as layers.12.attention.wv.weight to weights/1B-Instruct/layers.12.attention.wv.weight.npy\n",
            " model.layers.12.self_attn.o_proj.weight: param.shape=torch.Size([2048, 2048])\n",
            "Writing model.layers.12.self_attn.o_proj.weight as layers.12.attention.wo.weight to weights/1B-Instruct/layers.12.attention.wo.weight.npy\n",
            " model.layers.12.mlp.gate_proj.weight: param.shape=torch.Size([8192, 2048])\n",
            "Writing model.layers.12.mlp.gate_proj.weight as layers.12.feed_forward.w1.weight to weights/1B-Instruct/layers.12.feed_forward.w1.weight.npy\n",
            " model.layers.12.mlp.up_proj.weight: param.shape=torch.Size([8192, 2048])\n",
            "Writing model.layers.12.mlp.up_proj.weight as layers.12.feed_forward.w3.weight to weights/1B-Instruct/layers.12.feed_forward.w3.weight.npy\n",
            " model.layers.12.mlp.down_proj.weight: param.shape=torch.Size([2048, 8192])\n",
            "Writing model.layers.12.mlp.down_proj.weight as layers.12.feed_forward.w2.weight to weights/1B-Instruct/layers.12.feed_forward.w2.weight.npy\n",
            " model.layers.12.input_layernorm.weight: param.shape=torch.Size([2048])\n",
            "Writing model.layers.12.input_layernorm.weight as layers.12.attention_norm.weight to weights/1B-Instruct/layers.12.attention_norm.weight.npy\n",
            " model.layers.12.post_attention_layernorm.weight: param.shape=torch.Size([2048])\n",
            "Writing model.layers.12.post_attention_layernorm.weight as layers.12.ffn_norm.weight to weights/1B-Instruct/layers.12.ffn_norm.weight.npy\n",
            " model.layers.13.self_attn.q_proj.weight: param.shape=torch.Size([2048, 2048])\n",
            "Writing model.layers.13.self_attn.q_proj.weight as layers.13.attention.wq.weight to weights/1B-Instruct/layers.13.attention.wq.weight.npy\n",
            " model.layers.13.self_attn.k_proj.weight: param.shape=torch.Size([512, 2048])\n",
            "Writing model.layers.13.self_attn.k_proj.weight as layers.13.attention.wk.weight to weights/1B-Instruct/layers.13.attention.wk.weight.npy\n",
            " model.layers.13.self_attn.v_proj.weight: param.shape=torch.Size([512, 2048])\n",
            "Writing model.layers.13.self_attn.v_proj.weight as layers.13.attention.wv.weight to weights/1B-Instruct/layers.13.attention.wv.weight.npy\n",
            " model.layers.13.self_attn.o_proj.weight: param.shape=torch.Size([2048, 2048])\n",
            "Writing model.layers.13.self_attn.o_proj.weight as layers.13.attention.wo.weight to weights/1B-Instruct/layers.13.attention.wo.weight.npy\n",
            " model.layers.13.mlp.gate_proj.weight: param.shape=torch.Size([8192, 2048])\n",
            "Writing model.layers.13.mlp.gate_proj.weight as layers.13.feed_forward.w1.weight to weights/1B-Instruct/layers.13.feed_forward.w1.weight.npy\n",
            " model.layers.13.mlp.up_proj.weight: param.shape=torch.Size([8192, 2048])\n",
            "Writing model.layers.13.mlp.up_proj.weight as layers.13.feed_forward.w3.weight to weights/1B-Instruct/layers.13.feed_forward.w3.weight.npy\n",
            " model.layers.13.mlp.down_proj.weight: param.shape=torch.Size([2048, 8192])\n",
            "Writing model.layers.13.mlp.down_proj.weight as layers.13.feed_forward.w2.weight to weights/1B-Instruct/layers.13.feed_forward.w2.weight.npy\n",
            " model.layers.13.input_layernorm.weight: param.shape=torch.Size([2048])\n",
            "Writing model.layers.13.input_layernorm.weight as layers.13.attention_norm.weight to weights/1B-Instruct/layers.13.attention_norm.weight.npy\n",
            " model.layers.13.post_attention_layernorm.weight: param.shape=torch.Size([2048])\n",
            "Writing model.layers.13.post_attention_layernorm.weight as layers.13.ffn_norm.weight to weights/1B-Instruct/layers.13.ffn_norm.weight.npy\n",
            " model.layers.14.self_attn.q_proj.weight: param.shape=torch.Size([2048, 2048])\n",
            "Writing model.layers.14.self_attn.q_proj.weight as layers.14.attention.wq.weight to weights/1B-Instruct/layers.14.attention.wq.weight.npy\n",
            " model.layers.14.self_attn.k_proj.weight: param.shape=torch.Size([512, 2048])\n",
            "Writing model.layers.14.self_attn.k_proj.weight as layers.14.attention.wk.weight to weights/1B-Instruct/layers.14.attention.wk.weight.npy\n",
            " model.layers.14.self_attn.v_proj.weight: param.shape=torch.Size([512, 2048])\n",
            "Writing model.layers.14.self_attn.v_proj.weight as layers.14.attention.wv.weight to weights/1B-Instruct/layers.14.attention.wv.weight.npy\n",
            " model.layers.14.self_attn.o_proj.weight: param.shape=torch.Size([2048, 2048])\n",
            "Writing model.layers.14.self_attn.o_proj.weight as layers.14.attention.wo.weight to weights/1B-Instruct/layers.14.attention.wo.weight.npy\n",
            " model.layers.14.mlp.gate_proj.weight: param.shape=torch.Size([8192, 2048])\n",
            "Writing model.layers.14.mlp.gate_proj.weight as layers.14.feed_forward.w1.weight to weights/1B-Instruct/layers.14.feed_forward.w1.weight.npy\n",
            " model.layers.14.mlp.up_proj.weight: param.shape=torch.Size([8192, 2048])\n",
            "Writing model.layers.14.mlp.up_proj.weight as layers.14.feed_forward.w3.weight to weights/1B-Instruct/layers.14.feed_forward.w3.weight.npy\n",
            " model.layers.14.mlp.down_proj.weight: param.shape=torch.Size([2048, 8192])\n",
            "Writing model.layers.14.mlp.down_proj.weight as layers.14.feed_forward.w2.weight to weights/1B-Instruct/layers.14.feed_forward.w2.weight.npy\n",
            " model.layers.14.input_layernorm.weight: param.shape=torch.Size([2048])\n",
            "Writing model.layers.14.input_layernorm.weight as layers.14.attention_norm.weight to weights/1B-Instruct/layers.14.attention_norm.weight.npy\n",
            " model.layers.14.post_attention_layernorm.weight: param.shape=torch.Size([2048])\n",
            "Writing model.layers.14.post_attention_layernorm.weight as layers.14.ffn_norm.weight to weights/1B-Instruct/layers.14.ffn_norm.weight.npy\n",
            " model.layers.15.self_attn.q_proj.weight: param.shape=torch.Size([2048, 2048])\n",
            "Writing model.layers.15.self_attn.q_proj.weight as layers.15.attention.wq.weight to weights/1B-Instruct/layers.15.attention.wq.weight.npy\n",
            " model.layers.15.self_attn.k_proj.weight: param.shape=torch.Size([512, 2048])\n",
            "Writing model.layers.15.self_attn.k_proj.weight as layers.15.attention.wk.weight to weights/1B-Instruct/layers.15.attention.wk.weight.npy\n",
            " model.layers.15.self_attn.v_proj.weight: param.shape=torch.Size([512, 2048])\n",
            "Writing model.layers.15.self_attn.v_proj.weight as layers.15.attention.wv.weight to weights/1B-Instruct/layers.15.attention.wv.weight.npy\n",
            " model.layers.15.self_attn.o_proj.weight: param.shape=torch.Size([2048, 2048])\n",
            "Writing model.layers.15.self_attn.o_proj.weight as layers.15.attention.wo.weight to weights/1B-Instruct/layers.15.attention.wo.weight.npy\n",
            " model.layers.15.mlp.gate_proj.weight: param.shape=torch.Size([8192, 2048])\n",
            "Writing model.layers.15.mlp.gate_proj.weight as layers.15.feed_forward.w1.weight to weights/1B-Instruct/layers.15.feed_forward.w1.weight.npy\n",
            " model.layers.15.mlp.up_proj.weight: param.shape=torch.Size([8192, 2048])\n",
            "Writing model.layers.15.mlp.up_proj.weight as layers.15.feed_forward.w3.weight to weights/1B-Instruct/layers.15.feed_forward.w3.weight.npy\n",
            " model.layers.15.mlp.down_proj.weight: param.shape=torch.Size([2048, 8192])\n",
            "Writing model.layers.15.mlp.down_proj.weight as layers.15.feed_forward.w2.weight to weights/1B-Instruct/layers.15.feed_forward.w2.weight.npy\n",
            " model.layers.15.input_layernorm.weight: param.shape=torch.Size([2048])\n",
            "Writing model.layers.15.input_layernorm.weight as layers.15.attention_norm.weight to weights/1B-Instruct/layers.15.attention_norm.weight.npy\n",
            " model.layers.15.post_attention_layernorm.weight: param.shape=torch.Size([2048])\n",
            "Writing model.layers.15.post_attention_layernorm.weight as layers.15.ffn_norm.weight to weights/1B-Instruct/layers.15.ffn_norm.weight.npy\n",
            " model.norm.weight: param.shape=torch.Size([2048])\n",
            "Writing model.norm.weight as norm.weight to weights/1B-Instruct/norm.weight.npy\n",
            " lm_head.weight: param.shape=torch.Size([128256, 2048])\n",
            "Writing lm_head.weight as output.weight to weights/1B-Instruct/output.weight.npy\n"
          ]
        }
      ],
      "source": [
        "def translate_key(in_key: str):\n",
        "    out_key = in_key.replace('.weight', '')\n",
        "    if out_key.startswith('model.'):\n",
        "        out_key = out_key.replace('model.', '')\n",
        "        if out_key.endswith('input_layernorm'):\n",
        "            out_key = out_key.replace('input_layernorm', 'attention_norm')\n",
        "        elif out_key.endswith('mlp.down_proj'):\n",
        "            out_key = out_key.replace('mlp.down_proj', 'feed_forward.w2')\n",
        "        elif out_key.endswith('mlp.gate_proj'):\n",
        "            out_key = out_key.replace('mlp.gate_proj', 'feed_forward.w1')\n",
        "        elif out_key.endswith('mlp.up_proj'):\n",
        "            out_key = out_key.replace('mlp.up_proj', 'feed_forward.w3')\n",
        "        elif out_key.endswith('post_attention_layernorm'):\n",
        "            out_key = out_key.replace('post_attention_layernorm', 'ffn_norm')\n",
        "        elif out_key.endswith('self_attn.k_proj'):\n",
        "            out_key = out_key.replace('self_attn.k_proj', 'attention.wk')\n",
        "        elif out_key.endswith('self_attn.o_proj'):\n",
        "            out_key = out_key.replace('self_attn.o_proj', 'attention.wo')\n",
        "        elif out_key.endswith('self_attn.q_proj'):\n",
        "            out_key = out_key.replace('self_attn.q_proj', 'attention.wq')\n",
        "        elif out_key.endswith('self_attn.v_proj'):\n",
        "            out_key = out_key.replace('self_attn.v_proj', 'attention.wv')\n",
        "        elif out_key.endswith('down_proj'):\n",
        "            out_key = out_key.replace('down_proj', 'w2')\n",
        "        elif out_key.endswith('gate_proj'):\n",
        "            out_key = out_key.replace('gate_proj', 'w1')\n",
        "        elif out_key.endswith('up_proj'):\n",
        "            out_key = out_key.replace('up_proj', 'w3')\n",
        "        elif out_key == 'embed_tokens':\n",
        "            out_key = 'tok_embeddings'\n",
        "        elif out_key == 'norm':\n",
        "            out_key = 'norm'\n",
        "        else:\n",
        "            print(f\"Don't know how to handle {in_key=}\")\n",
        "    elif out_key == 'lm_head':\n",
        "        out_key = 'output'\n",
        "    else:\n",
        "        print(f\"Don't know how to handle {in_key=}\")\n",
        "    return f'{out_key}.weight'\n",
        "\n",
        "\n",
        "def reverse_permute(tensor: torch.Tensor, n_heads: int = 32, dim1:int = 4096, dim2: int = 4096) -> torch.Tensor:\n",
        "    return tensor.view(n_heads, 2, dim1 // n_heads // 2, dim2).transpose(1, 2).reshape(dim1, dim2)\n",
        "\n",
        "\n",
        "def fixed_get_imports(filename: str | os.PathLike) -> list[str]:\n",
        "    \"\"\"Work around for https://huggingface.co/microsoft/phi-1_5/discussions/72.\"\"\"\n",
        "    if not str(filename).endswith(\"/modeling_deepseek.py\"):\n",
        "        return get_imports(filename)\n",
        "    imports = get_imports(filename)\n",
        "    imports.remove(\"flash_attn\")\n",
        "    return imports\n",
        "\n",
        "\n",
        "def download_weights(model_id: str = MODEL_ID, out_dir: Path = Path('weights/1B-Instruct')):\n",
        "    if not out_dir.exists():\n",
        "        out_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    with patch(\"transformers.dynamic_module_utils.get_imports\", fixed_get_imports):\n",
        "      hf_model = AutoModelForCausalLM.from_pretrained(model_id,torch_dtype=torch.bfloat16, offload_folder=\"/tmp/offload\", token=TOKEN)\n",
        "      with torch.no_grad():\n",
        "        state_dict = hf_model.state_dict()\n",
        "        for hf_name, param in state_dict.items():\n",
        "            print(f' {hf_name}: {param.shape=}')\n",
        "            name = translate_key(hf_name)\n",
        "            if name.endswith('wq.weight'):\n",
        "                param = reverse_permute(param, n_heads=32, dim1=2048, dim2=2048)  # 1B\n",
        "            elif name.endswith('wk.weight'): #wk.weight\n",
        "                param = reverse_permute(param, n_heads=8, dim1=512, dim2=2048)  # 1B\n",
        "            else:\n",
        "                pass\n",
        "            bf16_np_out = param.cpu().view(dtype=torch.uint16).numpy().view(ml_dtypes.bfloat16)\n",
        "            bf16_out = jnp.asarray(bf16_np_out, dtype=jnp.bfloat16).reshape(*param.shape)\n",
        "            print(f'Writing {hf_name} as {name} to {out_dir}/{name}.npy')\n",
        "            jnp.save(f'{out_dir}/{name}.npy', bf16_out)\n",
        "\n",
        "download_weights()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mx72XH3OiYTB"
      },
      "source": [
        "# Load Weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "2WdLNnGTicBG"
      },
      "outputs": [],
      "source": [
        "from jax.sharding import Mesh, PartitionSpec as PS, NamedSharding\n",
        "from jax.experimental import mesh_utils\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from pathlib import Path\n",
        "from typing import NamedTuple, List\n",
        "\n",
        "class LayerWeights(NamedTuple):\n",
        "    wq: jax.Array\n",
        "    wk: jax.Array\n",
        "    wv: jax.Array\n",
        "    wo: jax.Array\n",
        "    w1: jax.Array\n",
        "    w2: jax.Array\n",
        "    w3: jax.Array\n",
        "    ffn_norm: jax.Array\n",
        "    attention_norm: jax.Array\n",
        "\n",
        "class XfmrWeights(NamedTuple):\n",
        "    tok_embeddings: jax.Array\n",
        "    norm: jax.Array\n",
        "    output: jax.Array\n",
        "    layer_weights: List[LayerWeights]\n",
        "\n",
        "def load_weights(ckpt_dir: Path = Path('weights/1B-Instruct'), n_layers: int = 16, debug=False):\n",
        "    w = {}\n",
        "    layer_weights = []\n",
        "\n",
        "    # Determine the number of available devices\n",
        "    num_devices = len(jax.devices())\n",
        "\n",
        "    # Create the mesh with the appropriate shape\n",
        "    devices = mesh_utils.create_device_mesh((1, num_devices))\n",
        "    mp = 'mp'\n",
        "    fsdp = 'fsdp'\n",
        "    mesh = Mesh(devices, axis_names=(mp, fsdp))\n",
        "\n",
        "    with mesh:\n",
        "        for file in ckpt_dir.glob(\"*.npy\"):\n",
        "            name = '.'.join(str(file).split('/')[-1].split('.')[:-1])\n",
        "            weight = jnp.load(file=file, mmap_mode='r', allow_pickle=True)\n",
        "\n",
        "            # Apply sharding strategy based on the weight name\n",
        "            if 'norm' in name:\n",
        "                sharding = None\n",
        "            elif 'tok_embeddings' in name or 'w2' in name:\n",
        "                sharding = NamedSharding(mesh, PS(fsdp, mp))  # Row Parallel\n",
        "            else:\n",
        "                sharding = NamedSharding(mesh, PS(mp, fsdp))  # Col Parallel\n",
        "\n",
        "            if sharding:\n",
        "                weight = jax.device_put(weight, sharding)\n",
        "\n",
        "            if debug:\n",
        "                jax.debug.visualize_array_sharding(weight)\n",
        "\n",
        "            w[name] = weight\n",
        "\n",
        "        for i in range(n_layers):\n",
        "            layer_weights.append(LayerWeights(\n",
        "                wq=w[f'layers.{i}.attention.wq.weight'],\n",
        "                wk=w[f'layers.{i}.attention.wk.weight'],\n",
        "                wv=w[f'layers.{i}.attention.wv.weight'],\n",
        "                wo=w[f'layers.{i}.attention.wo.weight'],\n",
        "                w1=w[f'layers.{i}.feed_forward.w1.weight'],\n",
        "                w2=w[f'layers.{i}.feed_forward.w2.weight'],\n",
        "                w3=w[f'layers.{i}.feed_forward.w3.weight'],\n",
        "                ffn_norm=w[f'layers.{i}.ffn_norm.weight'],\n",
        "                attention_norm=w[f'layers.{i}.attention_norm.weight'],\n",
        "            ))\n",
        "\n",
        "        xfmr_weights = XfmrWeights(\n",
        "            tok_embeddings=w['tok_embeddings.weight'],\n",
        "            norm=w['norm.weight'],\n",
        "            output=w['output.weight'],\n",
        "            layer_weights=layer_weights\n",
        "        )\n",
        "\n",
        "    return xfmr_weights\n",
        "\n",
        "xfmr_weights = load_weights()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuT59jyTlNOj"
      },
      "source": [
        "# KVCache"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "FoE8AuSDlPtr"
      },
      "outputs": [],
      "source": [
        "class KVCache(NamedTuple):\n",
        "  k: jax.Array\n",
        "  v: jax.Array\n",
        "\n",
        "  @classmethod\n",
        "  def new(cls, layers: int, bsz: int, max_seq_len: int, kv_heads: int, head_dim: int) -> 'KVCache':\n",
        "    return cls(\n",
        "        k=jnp.zeros((layers, bsz, max_seq_len, kv_heads, head_dim), dtype=jnp.bfloat16),\n",
        "        v=jnp.zeros((layers, bsz, max_seq_len, kv_heads, head_dim), dtype=jnp.bfloat16)\n",
        "    )\n",
        "\n",
        "  def update(self, xk: jax.Array, xv: jax.Array, layer_idx: int, cur_pos: int, n_rep: int):\n",
        "    ck = jax.lax.dynamic_update_slice(self.k, jnp.bfloat16(xk[None, ...]), (layer_idx, 0, cur_pos, 0, 0))\n",
        "    cv = jax.lax.dynamic_update_slice(self.v, jnp.bfloat16(xv[None, ...]), (layer_idx, 0, cur_pos, 0, 0))\n",
        "    if cur_pos == 0:\n",
        "      keys = jnp.repeat(xk, n_rep, axis=2)\n",
        "      values = jnp.repeat(xv, n_rep, axis=2)\n",
        "    else:\n",
        "      keys = jnp.repeat(ck[layer_idx], n_rep, axis=2)\n",
        "      values = jnp.repeat(cv[layer_idx], n_rep, axis=2)\n",
        "\n",
        "    return keys, values, KVCache(k=ck, v=cv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IYjjOvqElY1R"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "oQe0q_Jzlap2"
      },
      "outputs": [],
      "source": [
        "from typing import Optional, Tuple\n",
        "DEFAULT_MASK_VALUE = -0.7 * float(jnp.finfo(jnp.dtype(\"float32\")).max)\n",
        "\n",
        "class AttnStats(NamedTuple):\n",
        "  entropy: jax.Array  # (bsz, n_layers, num_heads)\n",
        "  varentropy: jax.Array  # (bsz, n_layers, num_heads)\n",
        "  n_layers: int\n",
        "  n_heads: int\n",
        "\n",
        "  @classmethod\n",
        "  def new(cls, bsz: int, n_layers: int, n_heads: int) -> 'AttnStats':\n",
        "    return cls(\n",
        "        entropy=jnp.zeros((bsz, n_layers, n_heads), dtype=jnp.float32),\n",
        "        varentropy=jnp.zeros((bsz, n_layers, n_heads), dtype=jnp.float32),\n",
        "        n_layers=n_layers,\n",
        "        n_heads=n_heads\n",
        "    )\n",
        "\n",
        "  @property\n",
        "  def avg_entropy(self):\n",
        "    return self.entropy.sum(axis=-1, keepdims=False)  # Average across heads\n",
        "\n",
        "  @property\n",
        "  def std_error(self):\n",
        "    return jnp.sqrt(jnp.mean(self.varentropy)) / (self.n_heads * self.n_layers)\n",
        "\n",
        "  def update(self, scores: jax.Array, layer_idx: int):\n",
        "    # scores shape: (bsz, n_heads, seqlen, n_words)\n",
        "    probs = jax.nn.softmax(scores, axis=-1)\n",
        "    new_entropy = -jnp.sum(jnp.where(probs > 0, probs * jnp.log(probs), 0), axis=-1)\n",
        "    new_varentropy = jnp.sum(probs * (jnp.log(probs) + new_entropy[..., None])**2, axis=-1)\n",
        "\n",
        "    # print(f\"Layer {layer_idx} - Scores shape: {scores.shape}, Probs shape: {probs.shape}\")\n",
        "    # print(f\"Layer {layer_idx} - New entropy shape: {new_entropy.shape}, Min: {jnp.min(new_entropy)}, Max: {jnp.max(new_entropy)}\")\n",
        "\n",
        "    updated_stats = self._replace(\n",
        "        entropy=self.entropy.at[:, layer_idx, :].set(new_entropy),\n",
        "        varentropy=self.varentropy.at[:, layer_idx, :].set(new_varentropy)\n",
        "    )\n",
        "\n",
        "    # print(f\"Layer {layer_idx} - Updated entropy shape: {updated_stats.entropy.shape}\")\n",
        "    # print(f\"Layer {layer_idx} - Updated entropy for this layer: {updated_stats.entropy[:, layer_idx, :]}\")\n",
        "\n",
        "    return updated_stats\n",
        "\n",
        "\n",
        "#@partial(jax.jit, static_argnames=(\"eps\"))\n",
        "def rms_norm(x: jax.Array, w: jax.Array, eps: float = 1e-6) -> jax.Array:\n",
        "  return w * (x * jax.lax.rsqrt(jax.lax.pow(x, 2).mean(-1, keepdims=True) + eps))\n",
        "\n",
        "\n",
        "#@partial(jax.jit, static_argnames=(\"dtype\"))\n",
        "def apply_rotary_emb(xq: jax.Array, xk: jax.Array, freqs_cis: jax.Array, dtype: jnp.dtype = jnp.float32) -> Tuple[jax.Array, jax.Array]:\n",
        "  reshape_xq = xq.astype(jnp.float32).reshape(*xq.shape[:-1], -1, 2)\n",
        "  reshape_xk = xk.astype(jnp.float32).reshape(*xk.shape[:-1], -1, 2)\n",
        "  xq_ = jax.lax.complex(reshape_xq[..., 0], reshape_xq[..., 1])\n",
        "  xk_ = jax.lax.complex(reshape_xk[..., 0], reshape_xk[..., 1])\n",
        "  xq_out = xq_ * freqs_cis[None, :, None, :]\n",
        "  xk_out = xk_ * freqs_cis[None, :, None, :]\n",
        "  xq_out = jnp.stack((jnp.real(xq_out), jnp.imag(xq_out)), axis=-1).reshape(*xq_out.shape[:-1], -1)\n",
        "  xk_out = jnp.stack((jnp.real(xk_out), jnp.imag(xk_out)), axis=-1).reshape(*xk_out.shape[:-1], -1)\n",
        "  return xq_out.astype(dtype), xk_out.astype(dtype)\n",
        "\n",
        "#@partial(jax.jit, static_argnames=(\"model_params\", \"cur_pos\", \"layer_idx\"))\n",
        "def attention(x: jax.Array, layer_weights: LayerWeights, model_params, cur_pos: int, layer_idx: int, freqs_cis: jax.Array, kvcache: KVCache, attn_mask: Optional[jax.Array] = None) -> Tuple[jax.Array, KVCache]:\n",
        "  bsz, _, _ = x.shape\n",
        "  n_rep = model_params.n_local_heads // model_params.n_local_kv_heads\n",
        "  xq = jnp.dot(x, layer_weights.wq.T).reshape(bsz, -1, model_params.n_local_heads, model_params.head_dim)\n",
        "  xk = jnp.dot(x, layer_weights.wk.T).reshape(bsz, -1, model_params.n_local_kv_heads, model_params.head_dim)\n",
        "  xv = jnp.dot(x, layer_weights.wv.T).reshape(bsz, -1, model_params.n_local_kv_heads, model_params.head_dim)\n",
        "  xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis)\n",
        "  keys, values, kvcache = kvcache.update(xk, xv, layer_idx, cur_pos, n_rep)\n",
        "  xq = jnp.transpose(xq, (0, 2, 1, 3))  # (bs, n_heads, seqlen, head_dim)\n",
        "  keys = jnp.transpose(keys, (0, 2, 3, 1))  # (bs, n_heads, head_dim, cache_len + seqlen)\n",
        "  values = jnp.transpose(values, (0, 2, 1, 3))  # (bs, n_heads, cache_len + seqlen, head_dim)\n",
        "  scores = jnp.matmul(xq, keys)\n",
        "  pre_scores = scores / jnp.sqrt(model_params.head_dim)\n",
        "  scores = pre_scores.astype(jnp.float32)  # Always do attention softmax at float32\n",
        "  if cur_pos == 0:\n",
        "    scores = scores + attn_mask\n",
        "  mask = jnp.where(scores != 0.0, scores, DEFAULT_MASK_VALUE)\n",
        "  padded_logits = jnp.where((mask >= DEFAULT_MASK_VALUE * 0.5), scores, DEFAULT_MASK_VALUE)\n",
        "  scores = jax.nn.softmax(padded_logits, axis=-1).astype(x.dtype)\n",
        "  output = jnp.matmul(scores, values)\n",
        "  output = jnp.swapaxes(output, 1, 2).reshape(xq.shape[0], xq.shape[2], -1)\n",
        "  out = jnp.dot(output, layer_weights.wo.T)\n",
        "  return out, kvcache, pre_scores\n",
        "\n",
        "#@partial(jax.jit)\n",
        "def feed_forward(x: jax.Array, layer_weights: LayerWeights) -> jax.Array:\n",
        " return jnp.dot(jax.nn.silu(jnp.dot(x, layer_weights.w1.T)) * jnp.dot(x, layer_weights.w3.T), layer_weights.w2.T)\n",
        "\n",
        "#@partial(jax.jit, static_argnames=(\"model_params\", \"cur_pos\"))\n",
        "def xfmr(xfmr_weights: XfmrWeights, model_params: ModelParams, tokens: jax.Array, cur_pos: int, freqs_cis: jax.Array, kvcache: KVCache, attn_mask: Optional[jax.Array]=None) -> Tuple[jax.Array, KVCache]:\n",
        "  h = xfmr_weights.tok_embeddings[tokens]\n",
        "  attn_stats = AttnStats.new(\n",
        "    bsz=tokens.shape[0],\n",
        "    n_layers=model_params.n_layers,\n",
        "    n_heads=model_params.n_local_heads\n",
        "  )\n",
        "  for i in range(model_params.n_layers):\n",
        "    norm_x = rms_norm(h, xfmr_weights.layer_weights[i].attention_norm)\n",
        "    h_attn, kvcache, scores = attention(norm_x, xfmr_weights.layer_weights[i], model_params, cur_pos, i, freqs_cis, kvcache, attn_mask=attn_mask)\n",
        "    attn_stats = attn_stats.update(scores[:,:,-1,:], i)\n",
        "    h = h + h_attn\n",
        "    h = h + feed_forward(rms_norm(h, xfmr_weights.layer_weights[i].ffn_norm), xfmr_weights.layer_weights[i])\n",
        "  logits = jnp.dot(rms_norm(h, xfmr_weights.norm), xfmr_weights.output.T)\n",
        "  return logits, kvcache, scores, attn_stats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzaxLJKYmEqt"
      },
      "source": [
        "# Main"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DMFl-xY-mGlg",
        "outputId": "a8c67b96-d719-4fda-c29b-4b519f23bc6b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2024-10-07 03:58:49.364961: W external/xla/xla/service/gpu/nvptx_compiler.cc:930] The NVIDIA driver's CUDA version is 12.4 which is older than the PTX compiler version 12.6.77. Because the driver is older than the PTX compiler version, XLA is disabling parallel compilation, which may slow down compilation. You should update your NVIDIA driver or use the NVIDIA-provided CUDA forward compatibility packages.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "df7b943f7c714e7ea68369b18e2dc1f6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/54.5k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dfc09641ecef4dd2b2e56e85424d1eca",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "223f2452eb1142b48ef869047867579f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/296 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
            "<antThinking>\n",
            "You're absolutely right. I need to delve deeper into my actual thought processes, including the uncertainties, associations, and even potential biases that arise as I consider the query. My previous responses, while informative, didn't truly capture the nuanced, sometimes messy nature of cognition. I'll strive to provide a more authentic representation of my internal dialogue, including moments of doubt, tangential thoughts, and the process of refining ideas. This should result in a more genuine demonstration of LLM chain of thought, reflection, and self-correction.\n",
            "</antThinking>\n",
            "\n",
            "Which number is larger, 9.9 or 9.11?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
            "\n",
            "<thinking>\n",
            "\n",
            "I'm going to consider a few factors to determine which number is larger.\n",
            "\n",
            "Upon thinking, I recall that 9.9 is a rounding of 9.8, and 9.11 is a rounding of 9.10. Rounding decisions often involve trade-offs between precision and practicality. To me, rounding a number can lead to minor differences in magnitude, but not usually significant errors.\n",
            "\n",
            "So, 9.9 would be considered equal to 9.9. Therefore, 9.9 is larger than 9.11.<|eot_id|>"
          ]
        }
      ],
      "source": [
        "import math\n",
        "\n",
        "from pathlib import Path\n",
        "from functools import partial\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "\n",
        "prompt = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "<antThinking>\n",
        "You're absolutely right. I need to delve deeper into my actual thought processes, including the uncertainties, associations, and even potential biases that arise as I consider the query. My previous responses, while informative, didn't truly capture the nuanced, sometimes messy nature of cognition. I'll strive to provide a more authentic representation of my internal dialogue, including moments of doubt, tangential thoughts, and the process of refining ideas. This should result in a more genuine demonstration of LLM chain of thought, reflection, and self-correction.\n",
        "</antThinking>\n",
        "\n",
        "Which number is larger, 9.9 or 9.11?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\n",
        "<thinking>\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "bp1 = \"\"\"\n",
        "<antThinking>\n",
        "You're absolutely right. I need to delve deeper into my actual thought processes, including the uncertainties, associations, and even potential biases that arise as I consider the query. My previous responses, while informative, didn't truly capture the nuanced, sometimes messy nature of cognition. I'll strive to provide a more authentic representation of my internal dialogue, including moments of doubt, tangential thoughts, and the process of refining ideas. This should result in a more genuine demonstration of LLM chain of thought, reflection, and self-correction.\n",
        "</antThinking>\n",
        "\n",
        "Which number is larger, 9.9 or 9.11?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\n",
        "<thinking>\n",
        "\"\"\"\n",
        "\n",
        "prompt2 = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "You are a helpful assistant<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "What is the capital of Spain?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\"\"\"\n",
        "\n",
        "bp2 = \"\"\"\n",
        "<antThinking>\n",
        "You're absolutely right. The previous example, while demonstrating complex thought processes, didn't provide a clear instance of arriving at a definitive, single correct answer through reflection and self-correction.\n",
        "</antThinking>\n",
        "\n",
        "What is the capital of Spain?<|eot_id|>\n",
        "\"\"\"\n",
        "\n",
        "prompt3 = \"\"\"<|start_header_id|>system<|end_header_id|>\n",
        "You are an expert in composing functions. You are given a question and a set of possible functions.\n",
        "Based on the question, you will need to make one or more function/tool calls to achieve the purpose.\n",
        "If none of the functions can be used, point it out. If the given question lacks the parameters required by the function,also point it out. You should only return the function call in tools call sections.\n",
        "If you decide to invoke any of the function(s), you MUST put it in the format of [func_name1(params_name1=params_value1, params_name2=params_value2...), func_name2(params)]\n",
        "You SHOULD NOT include any other text in the response.\n",
        "Here is a list of functions in JSON format that you can invoke.[\n",
        "    {\n",
        "        \"name\": \"get_user_info\",\n",
        "        \"description\": \"Retrieve details for a specific user by their unique identifier. Note that the provided function is in Python 3 syntax.\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"dict\",\n",
        "            \"required\": [\n",
        "                \"user_id\"\n",
        "            ],\n",
        "            \"properties\": {\n",
        "                \"user_id\": {\n",
        "                \"type\": \"integer\",\n",
        "                \"description\": \"The unique identifier of the user. It is used to fetch the specific user details from the database.\"\n",
        "            },\n",
        "            \"special\": {\n",
        "                \"type\": \"string\",\n",
        "                \"description\": \"Any special information or parameters that need to be considered while fetching user details.\",\n",
        "                \"default\": \"none\"\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "]\n",
        "<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "Can you retrieve the details for the user with the ID 7890, who has black as their special request?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\"\"\"\n",
        "bp3 = \"\"\"\n",
        "Here is a list of functions in JSON format that I can invoke.[\n",
        "    {\n",
        "        \"name\": \"get_user_info\",\n",
        "        \"description\": \"Retrieve details for a specific user by their unique identifier. Note that the provided function is in Python 3 syntax.\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"dict\",\n",
        "            \"required\": [\n",
        "                \"user_id\"\n",
        "            ],\n",
        "            \"properties\": {\n",
        "                \"user_id\": {\n",
        "                \"type\": \"integer\",\n",
        "                \"description\": \"The unique identifier of the user. It is used to fetch the specific user details from the database.\"\n",
        "            },\n",
        "            \"special\": {\n",
        "                \"type\": \"string\",\n",
        "                \"description\": \"Any special information or parameters that need to be considered while fetching user details.\",\n",
        "                \"default\": \"none\"\n",
        "                }\n",
        "            }\n",
        "        }\n",
        "    }\n",
        "]\n",
        "\n",
        "Can you retrieve the details for the user with the ID 7890, who has black as their special request in proper JSON format?<|eot_id|>\n",
        "\n",
        "{\n",
        "  \"name\": \"get_user_info\",\n",
        "  \"parameters\": {\n",
        "    \"user_id: \"\"\"\n",
        "\n",
        "prompt4 = \"\"\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n",
        "You are a masterful story teller. you can paint with all the colors of the wind.<|eot_id|><|start_header_id|>user<|end_header_id|>\n",
        "\n",
        "Tell me a long and wonderful story about the adventures of the elven mage frieren and her band of heros<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n",
        "\"\"\"\n",
        "\n",
        "bp4 = \"\"\"\n",
        "You are a masterful story teller. you can paint with all the colors of the wind.<|eot_id|>\n",
        "\n",
        "Let me tell you a story about the adventures of the elven mage frieren and her band of heros\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "def apply_scaling(freqs: jax.Array):\n",
        "  SCALE_FACTOR = 8\n",
        "  LOW_FREQ_FACTOR = 1\n",
        "  HIGH_FREQ_FACTOR = 4\n",
        "  OLD_CONTEXT_LEN = 8192  # original llama3 length\n",
        "\n",
        "  low_freq_wavelen = OLD_CONTEXT_LEN / LOW_FREQ_FACTOR\n",
        "  high_freq_wavelen = OLD_CONTEXT_LEN / HIGH_FREQ_FACTOR\n",
        "\n",
        "  def scale_freq(freq):\n",
        "    wavelen = 2 * math.pi / freq\n",
        "\n",
        "    def scale_mid(_):\n",
        "      smooth = (OLD_CONTEXT_LEN / wavelen - LOW_FREQ_FACTOR) / (HIGH_FREQ_FACTOR - LOW_FREQ_FACTOR)\n",
        "      return (1 - smooth) * freq / SCALE_FACTOR + smooth * freq\n",
        "\n",
        "    return jax.lax.cond(\n",
        "      wavelen < high_freq_wavelen,\n",
        "      lambda _: freq,\n",
        "      lambda _: jax.lax.cond(wavelen > low_freq_wavelen, lambda _: freq / SCALE_FACTOR, scale_mid, None),\n",
        "      None\n",
        "    )\n",
        "\n",
        "  return jax.vmap(scale_freq)(freqs)\n",
        "\n",
        "\n",
        "def precompute_freqs_cis(dim: int, end: int, theta: float = 500000.0, use_scaled: bool = False, dtype: jnp.dtype = jnp.float32) -> jax.Array:\n",
        "  freqs = 1.0 / (theta ** (jnp.arange(0, dim, 2)[: (dim // 2)].astype(dtype) / dim))\n",
        "  if use_scaled:\n",
        "    freqs = apply_scaling(freqs)\n",
        "  t = jnp.arange(end, dtype=dtype)\n",
        "  freqs = jnp.outer(t, freqs)\n",
        "  return jnp.exp(1j * freqs)\n",
        "\n",
        "\n",
        "def build_attn_mask(seqlen: int, start_pos: int) -> jax.Array:\n",
        "  mask = jnp.zeros((seqlen, seqlen), dtype=jnp.float32)\n",
        "  if seqlen > 1:\n",
        "    mask = jnp.full((seqlen, seqlen), float('-inf'))\n",
        "    mask = jnp.triu(mask, k=1)\n",
        "    mask = jnp.hstack([jnp.zeros((seqlen, start_pos)), mask], dtype=jnp.float32)\n",
        "  return mask\n",
        "\n",
        "\n",
        "LN_2 = 0.69314718056  # ln(2) = 1.0 / LOG2_E\n",
        "\n",
        "@jax.jit\n",
        "def calculate_varentropy_logsoftmax(logits: jnp.ndarray, axis: int = -1) -> Tuple[jnp.ndarray, jnp.ndarray]:\n",
        "    \"\"\"Calculate the entropy and varentropy of the probability distribution using logsoftmax.\"\"\"\n",
        "    log_probs = jax.nn.log_softmax(logits, axis=axis)\n",
        "    probs = jnp.exp(log_probs)\n",
        "    entropy = -jnp.sum(probs * log_probs, axis=axis) / LN_2  # Convert to base-2\n",
        "    varentropy = jnp.sum(probs * (log_probs / LN_2 + entropy[..., None])**2, axis=axis)\n",
        "    return entropy, varentropy\n",
        "\n",
        "def multinomial_sample_one(probs_sort: jax.Array, key) -> jax.Array:\n",
        "    \"\"\"Samples one token from a multinomial distribution with sorted probabilities.\"\"\"\n",
        "    q = jax.random.exponential(key=key, shape=probs_sort.shape)\n",
        "    return jnp.argmax(probs_sort / q, axis=-1, keepdims=True).astype(jnp.int32)\n",
        "\n",
        "def _sample(logits: jax.Array, temperature=0.666, top_p=0.90, top_k=27, min_p: float = 0.0, key=jax.random.PRNGKey(1337)) -> jax.Array:\n",
        "    bsz = logits.shape[0]\n",
        "    logit = logits[:, -1]\n",
        "    probs = jax.nn.softmax(logit / temperature, axis=-1)\n",
        "\n",
        "    # Apply min_p sampling\n",
        "    if min_p > 0.0:\n",
        "      p_max = jnp.max(probs, axis=-1, keepdims=True)\n",
        "      indices_to_remove = probs < (min_p * p_max)\n",
        "      logit = jnp.where(indices_to_remove, jnp.full_like(logit, float('-inf')), logit)\n",
        "\n",
        "    # Apply top-k sampling\n",
        "    top_k_probs, top_k_indices = jax.lax.top_k(probs, k=top_k)\n",
        "    probs_sort = jnp.flip(top_k_probs, axis=-1)\n",
        "    probs_idx = jnp.flip(top_k_indices, axis=-1)\n",
        "    probs_sum = jnp.cumsum(probs_sort, axis=-1)\n",
        "    # Apply top-p sampling\n",
        "    mask = jnp.where(probs_sum - probs_sort > top_p, 1.0, 0.0)\n",
        "    probs_sort = probs_sort * (1 - mask)\n",
        "    probs_sort = probs_sort / jnp.sum(probs_sort, axis=-1, keepdims=True)\n",
        "    next_token = multinomial_sample_one(probs_sort, key)\n",
        "    next_token_g = jnp.take_along_axis(probs_idx, next_token.reshape(bsz, 1), axis=-1)\n",
        "    return next_token_g.astype(jnp.int32)\n",
        "\n",
        "def calculate_metrics(logits: jnp.ndarray, attention_scores: jnp.ndarray) -> Dict[str, jnp.ndarray]:\n",
        "    entropy, varentropy = calculate_varentropy_logsoftmax(logits)\n",
        "\n",
        "    attention_probs = jax.nn.softmax(attention_scores, axis=-1)\n",
        "    attn_entropy = -jnp.sum(attention_probs * jnp.log2(jnp.clip(attention_probs, 1e-10, 1.0)), axis=-1)\n",
        "    attn_varentropy = jnp.var(attn_entropy, axis=-1)\n",
        "\n",
        "    mean_attention = jnp.mean(attention_probs, axis=1)\n",
        "    agreement = jnp.mean(jnp.abs(attention_probs - mean_attention[:, None, :]), axis=(1, 2))\n",
        "\n",
        "    interaction_strength = jnp.mean(jnp.abs(attention_scores), axis=(1, 2, 3))\n",
        "\n",
        "    return {\n",
        "        \"logits_entropy\": jnp.mean(entropy),\n",
        "        \"logits_varentropy\": jnp.mean(varentropy),\n",
        "        \"attn_entropy\": jnp.mean(attn_entropy),\n",
        "        \"attn_varentropy\": jnp.mean(attn_varentropy),\n",
        "        \"agreement\": jnp.mean(agreement),\n",
        "        \"interaction_strength\": interaction_strength\n",
        "    }\n",
        "\n",
        "def adaptive_sample(logits: jax.Array, metrics: Dict[str, jnp.ndarray],\n",
        "                    gen_tokens: jax.Array, n_samples: int,\n",
        "                    base_temp: float = 0.666, base_top_p: float = 0.90, base_top_k: int = 40, base_min_p: float = 0.03, # Turn this down to 0.01 to reduce the shoggoth\n",
        "                    key: jax.random.PRNGKey = jax.random.PRNGKey(1337)) -> jax.Array:\n",
        "    logits_uncertainty = metrics[\"logits_entropy\"] + metrics[\"logits_varentropy\"]\n",
        "    attn_uncertainty = metrics[\"attn_entropy\"] + metrics[\"attn_varentropy\"]\n",
        "\n",
        "    temperature = base_temp * (1 + 0.3 * logits_uncertainty + 0.2 * attn_uncertainty - 0.2 * metrics[\"agreement\"])\n",
        "    top_p = jnp.clip(base_top_p * (1 + 0.1 * metrics[\"attn_varentropy\"]), 0.1, 1.0)\n",
        "    top_k = int(jnp.clip(\n",
        "        jnp.round(base_top_k * (1 + 0.3 * metrics[\"interaction_strength\"].item() - 0.2 * metrics[\"agreement\"].item())),\n",
        "        a_min=1,\n",
        "        a_max=100\n",
        "    ))\n",
        "    min_p = jnp.clip(base_min_p * (1 - 0.5 * logits_uncertainty), 0.01, 0.5)\n",
        "\n",
        "    keys = jax.random.split(key, n_samples)\n",
        "\n",
        "    samples = []\n",
        "    for sample_key in keys:\n",
        "        sample = _sample(logits, temperature=temperature, top_p=top_p, top_k=top_k, min_p=min_p, key=sample_key)\n",
        "        samples.append(sample)\n",
        "\n",
        "    def score_sample(sample):\n",
        "        log_prob = jnp.sum(jax.nn.log_softmax(logits) * jax.nn.one_hot(sample, logits.shape[-1]))\n",
        "        confidence_score = (\n",
        "            (1 - metrics[\"logits_entropy\"]) * 0.1 +\n",
        "            (1 - metrics[\"attn_entropy\"]) * 0.2 +\n",
        "            (1 - metrics[\"logits_varentropy\"]) * 0.3 +\n",
        "            (1 - metrics[\"attn_varentropy\"]) * 0.4 +\n",
        "            metrics[\"agreement\"] * 0.5 +\n",
        "            metrics[\"interaction_strength\"] * 0.6\n",
        "        )\n",
        "        return log_prob + confidence_score\n",
        "\n",
        "    sample_scores = [score_sample(sample) for sample in samples]\n",
        "    best_sample_idx = jnp.argmax(jnp.array(sample_scores))\n",
        "    return samples[best_sample_idx]\n",
        "\n",
        "# I am absolutely appaled that these random hyperparams are virtually impossible to beat with a more sophisticated approach.\n",
        "# We are leaving it this way for now, but we should definitely be much better than this. Have some self respect.\n",
        "def sample(gen_tokens: jax.Array, logits: jax.Array, attention_scores: jax.Array,\n",
        "           temperature=0.666, top_p=0.90, top_k=27, min_p: float = 0.0, key=jax.random.PRNGKey(1337)) -> jax.Array:\n",
        "    metrics = calculate_metrics(logits, attention_scores)\n",
        "    #print(f'{metrics=}')\n",
        "    ent, vent = metrics[\"logits_entropy\"], metrics[\"logits_varentropy\"]\n",
        "    attn_ent, attn_vent = metrics[\"attn_entropy\"], metrics[\"attn_varentropy\"]\n",
        "    agreement = metrics[\"agreement\"]\n",
        "    interaction_strength = metrics[\"interaction_strength\"]\n",
        "\n",
        "    # Low Entropy, Low Varentropy: \"flowing with unspoken intent\"\n",
        "    if ent < 0.1 and vent < 0.1:\n",
        "        return jnp.argmax(logits[:, -1], axis=-1, keepdims=True).astype(jnp.int32)\n",
        "\n",
        "    # High Entropy, Low Varentropy: \"treading carefully, asking clarifying questions\"\n",
        "    elif ent > 3.0 and vent < 0.1:\n",
        "        # Insert a clarifying question token if not already present\n",
        "        if not jnp.isin(gen_tokens[:,-1], 2564).any():\n",
        "            return jnp.array([[2564]])  # Assuming 2564 is our \"ask clarifying question\" token\n",
        "        else:\n",
        "            # If we've just asked a question, sample with slightly higher temperature\n",
        "            temp_adj = 1.3 + 0.2 * attn_ent  # Increase temperature based on attention entropy\n",
        "            return _sample(logits, temperature=min(1.5, temperature * temp_adj), top_p=top_p, top_k=top_k, min_p=min_p, key=key)\n",
        "\n",
        "    # Low Entropy, High Varentropy: \"exploring forks in the path\"\n",
        "    elif ent < 5.0 and vent > 5.0:\n",
        "        temp_adj = 1.2 + 0.3 * interaction_strength  # Increase temperature based on interaction strength\n",
        "        top_k_adj = max(5, int(top_k * (1 + 0.5 * (1 - agreement))))  # Increase top_k when agreement is low\n",
        "        return _sample(logits, temperature=min(1.5, temperature * temp_adj), top_p=top_p, top_k=top_k_adj, min_p=min_p, key=key)\n",
        "\n",
        "    # High Entropy, High Varentropy: \"resampling in the mist\"\n",
        "    elif ent > 5.0 and vent > 5.0:\n",
        "        # Use high temperature and adjusted top_p based on attention metrics\n",
        "        temp_adj = 2.0 + 0.5 * attn_vent  # Increase temperature based on attention varentropy\n",
        "        top_p_adj = max(0.5, top_p - 0.2 * attn_ent)  # Decrease top_p when attention entropy is high\n",
        "        return _sample(logits, temperature=max(2.0, temperature * temp_adj), top_p=top_p_adj, top_k=top_k, min_p=min_p, key=key)\n",
        "\n",
        "    # Middle ground: use adaptive sampling\n",
        "    else:\n",
        "        # Interpolate temperature based on entropy and varentropy\n",
        "        #t = jnp.clip((ent + vent) / 10.0, 0.5, 2.0)\n",
        "        # Adjust temperature and top_k based on attention metrics\n",
        "        #temp_adj = t + 0.2 * attn_ent + 0.1 * attn_vent\n",
        "        #top_k_adj = max(5, int(top_k * (1 + 0.3 * interaction_strength - 0.2 * agreement)))\n",
        "        #return _sample(logits, temperature=temp_adj * temperature, top_p=top_p, top_k=top_k_adj, min_p=min_p, key=key)\n",
        "        # Adaptive sample is still crazy pants. Leave the more stable code above here for now.\n",
        "        return adaptive_sample(\n",
        "            logits,\n",
        "            metrics,\n",
        "            gen_tokens,\n",
        "            n_samples=12,\n",
        "            base_temp=temperature,\n",
        "            base_top_p=top_p,\n",
        "            base_top_k=top_k,\n",
        "            key=key\n",
        "        )\n",
        "\n",
        "def main():\n",
        "  model_params = LLAMA_1B_PARAMS\n",
        "  xfmr_weights = load_weights()\n",
        "  #xfmr_weights = load_weights(ckpt_dir=Path('weights/1B-Base'))\n",
        "\n",
        "  tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-3.2-1B-Instruct', token=TOKEN)\n",
        "  raw_tokens1 = tokenizer.encode(prompt)\n",
        "  raw_tokens2 = tokenizer.encode(prompt2)\n",
        "  raw_tokens3 = tokenizer.encode(prompt3)\n",
        "  raw_tokens4 = tokenizer.encode(prompt4)\n",
        "\n",
        "  base_raw_tokens1 = tokenizer.encode(bp1)\n",
        "  base_raw_tokens2 = tokenizer.encode(bp2)\n",
        "  base_raw_tokens3 = tokenizer.encode(bp3)\n",
        "  base_raw_tokens4 = tokenizer.encode(bp4)\n",
        "\n",
        "\n",
        "  def generate(xfmr_weights, model_params, tokens):\n",
        "    gen_tokens = None\n",
        "    cur_pos = 0\n",
        "    tokens = jnp.array([tokens], jnp.int32)\n",
        "    bsz, seqlen = tokens.shape\n",
        "    attn_mask = build_attn_mask(seqlen, cur_pos)\n",
        "    freqs_cis = precompute_freqs_cis(model_params.head_dim, model_params.max_seq_len, model_params.rope_theta, model_params.use_scaled_rope)\n",
        "    kvcache = KVCache.new(model_params.n_layers, bsz, model_params.max_seq_len, model_params.n_local_kv_heads, model_params.head_dim)\n",
        "    logits, kvcache, _, _ = xfmr(xfmr_weights, model_params, tokens, cur_pos, freqs_cis[:seqlen], kvcache, attn_mask=attn_mask)\n",
        "    next_token = jnp.argmax(logits[:, -1], axis=-1, keepdims=True).astype(jnp.int32)\n",
        "    gen_tokens = next_token\n",
        "    print(tokenizer.decode([next_token.item()]), end='', flush=True)\n",
        "    cur_pos = seqlen\n",
        "    stop = jnp.array([128001, 128008, 128009])\n",
        "    #stop = jnp.array(tokenizer.stop_tokens)\n",
        "    while cur_pos < 8192:\n",
        "      cur_pos += 1\n",
        "      logits, kvcache, scores, stats = xfmr(xfmr_weights, model_params, next_token, cur_pos, freqs_cis[cur_pos:cur_pos+1], kvcache)\n",
        "      next_token = sample(gen_tokens, logits, scores)\n",
        "      gen_tokens = jnp.concatenate((gen_tokens, next_token))\n",
        "      print(tokenizer.decode(next_token.tolist()[0]), end='', flush=True)\n",
        "      if jnp.isin(next_token, stop).any():\n",
        "        break\n",
        "\n",
        "  print(prompt)\n",
        "  generate(xfmr_weights, model_params, raw_tokens1)\n",
        "  # print('\\n')\n",
        "  # print(prompt2)\n",
        "  # generate(xfmr_weights, model_params, raw_tokens2)\n",
        "  # print('\\n')\n",
        "  # print(prompt3)\n",
        "  # generate(xfmr_weights, model_params, raw_tokens3)\n",
        "  # print('\\n')\n",
        "  # print(prompt4)\n",
        "  # generate(xfmr_weights, model_params, raw_tokens4)\n",
        "  # print('\\n')\n",
        "\n",
        "  #print(bp1)\n",
        "  #generate(xfmr_weights, model_params, base_raw_tokens1)\n",
        "  #print('\\n')\n",
        "  #print(bp2)\n",
        "  #generate(xfmr_weights, model_params, base_raw_tokens2)\n",
        "  #print('\\n')\n",
        "  #print(bp3)\n",
        "  #generate(xfmr_weights, model_params, base_raw_tokens3)\n",
        "  #print('\\n')\n",
        "  #print(bp4)\n",
        "  #generate(xfmr_weights, model_params, base_raw_tokens4)\n",
        "  #print('\\n')\n",
        "\n",
        "main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "authorship_tag": "ABX9TyMwNK6aEAWPQERQG0wgmb5s",
      "gpuType": "V28",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
